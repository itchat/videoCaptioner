"""
ç³»ç»Ÿä¼˜åŒ–æ£€æµ‹å’Œé…ç½®å·¥å…·
æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿã€å¤šçº¿ç¨‹é…ç½®ç­‰æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
"""
import os
import platform
import multiprocessing
import subprocess
import json
from typing import Dict, List, Tuple, Optional


class SystemOptimizer:
    """ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ£€æµ‹å’Œé…ç½®"""
    
    def __init__(self):
        self.system_info = self._detect_system_info()
        self.optimization_report = {}
    
    def _detect_system_info(self) -> Dict:
        """æ£€æµ‹ç³»ç»ŸåŸºæœ¬ä¿¡æ¯"""
        info = {
            'platform': platform.system(),
            'architecture': platform.architecture()[0],
            'cpu_count': multiprocessing.cpu_count(),
            'machine': platform.machine(),
            'processor': platform.processor(),
        }
        
        if info['platform'] == 'Darwin':  # macOS
            try:
                # è·å–Macç¡¬ä»¶ä¿¡æ¯
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    info['cpu_model'] = result.stdout.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯Apple Silicon
                result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                      capture_output=True, text=True)
                info['is_apple_silicon'] = result.returncode == 0 and result.stdout.strip() == '1'
                
                # è·å–å†…å­˜ä¿¡æ¯
                result = subprocess.run(['sysctl', '-n', 'hw.memsize'], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    memory_bytes = int(result.stdout.strip())
                    info['memory_gb'] = round(memory_bytes / (1024**3), 1)
                    
            except Exception as e:
                print(f"Warning: Could not detect macOS hardware details: {e}")
        
        return info
    
    def check_hardware_acceleration(self) -> Dict:
        """æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿæ”¯æŒæƒ…å†µ"""
        acceleration_info = {
            'gpu_available': False,
            'metal_support': False,
            'videotoolbox_support': False,
            'recommendations': []
        }
        
        if self.system_info['platform'] == 'Darwin':  # macOS
            # æ£€æŸ¥Metalæ”¯æŒï¼ˆApple GPUåŠ é€Ÿï¼‰
            try:
                result = subprocess.run(['system_profiler', 'SPDisplaysDataType', '-json'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    display_data = json.loads(result.stdout)
                    for item in display_data.get('SPDisplaysDataType', []):
                        if 'sppci_model' in item:
                            acceleration_info['gpu_available'] = True
                            if 'Apple' in item.get('sppci_model', ''):
                                acceleration_info['metal_support'] = True
                            break
            except Exception as e:
                print(f"Warning: Could not detect GPU info: {e}")
            
            # VideoToolboxæ€»æ˜¯åœ¨macOSä¸Šå¯ç”¨
            acceleration_info['videotoolbox_support'] = True
            
            # ç”Ÿæˆå»ºè®®
            if acceleration_info['metal_support']:
                acceleration_info['recommendations'].append("âœ… Apple Silicon GPU detected - optimal performance expected")
            elif acceleration_info['gpu_available']:
                acceleration_info['recommendations'].append("âš ï¸ Intel GPU detected - moderate acceleration available")
            
            if acceleration_info['videotoolbox_support']:
                acceleration_info['recommendations'].append("âœ… VideoToolbox available for video processing")
        
        return acceleration_info
    
    def get_optimal_thread_config(self) -> Dict:
        """è·å–æœ€ä¼˜çº¿ç¨‹é…ç½®"""
        cpu_count = self.system_info['cpu_count']
        
        config = {
            'whisper_threads': min(8, cpu_count),  # Whisperå¤„ç†çº¿ç¨‹
            'translation_threads_openai': min(9, max(3, (cpu_count // 4) * 3)),  # OpenAIç¿»è¯‘çº¿ç¨‹ - å¢åŠ ä¸‰å€
            'translation_threads_google': min(15, max(6, (cpu_count // 2) * 3)),  # Googleç¿»è¯‘çº¿ç¨‹ - å¢åŠ ä¸‰å€
            'main_thread_pool': min(4, max(2, cpu_count // 2)),  # ä¸»çº¿ç¨‹æ± 
            'reasoning': []
        }
        
        # Apple Siliconä¼˜åŒ–
        if self.system_info.get('is_apple_silicon', False):
            config['whisper_threads'] = min(10, cpu_count)  # Apple Siliconå¯¹AIä»»åŠ¡ä¼˜åŒ–æ›´å¥½
            config['translation_threads_openai'] = min(18, (cpu_count // 3) * 4)  # Apple Siliconä¼˜åŒ–
            config['translation_threads_google'] = min(30, (cpu_count // 2) * 4)  # Apple Siliconä¼˜åŒ–
            config['reasoning'].append("ğŸ Apple Silicon detected - increased Whisper and translation threads for ML optimization")
        
        # é«˜æ ¸å¿ƒæ•°CPUä¼˜åŒ–
        if cpu_count >= 8:
            config['translation_threads_openai'] = min(15, (cpu_count // 3) * 3)  # è¿›ä¸€æ­¥å¢åŠ é«˜æ ¸å¿ƒCPUçš„ç¿»è¯‘çº¿ç¨‹
            config['translation_threads_google'] = min(24, (cpu_count // 2) * 3)  # è¿›ä¸€æ­¥å¢åŠ é«˜æ ¸å¿ƒCPUçš„ç¿»è¯‘çº¿ç¨‹
            config['reasoning'].append(f"ğŸš€ High-core CPU ({cpu_count} cores) - increased translation parallelism (3x boost)")
        elif cpu_count <= 4:
            config['translation_threads_openai'] = 6  # å³ä½¿åœ¨ä½æ ¸å¿ƒæ•°ä¹Ÿä¿æŒè¾ƒé«˜çš„çº¿ç¨‹æ•°
            config['translation_threads_google'] = 9  # å³ä½¿åœ¨ä½æ ¸å¿ƒæ•°ä¹Ÿä¿æŒè¾ƒé«˜çš„çº¿ç¨‹æ•°
            config['main_thread_pool'] = 2
            config['reasoning'].append(f"âš ï¸ Limited cores ({cpu_count}) - but still using boosted translation threads for better throughput")
        
        # å†…å­˜è€ƒè™‘
        memory_gb = self.system_info.get('memory_gb', 8)
        if memory_gb < 8:
            config['whisper_threads'] = min(4, config['whisper_threads'])
            config['main_thread_pool'] = min(2, config['main_thread_pool'])
            config['reasoning'].append(f"ğŸ’¾ Limited memory ({memory_gb}GB) - reduced threading to prevent swapping")
        elif memory_gb >= 16:
            config['reasoning'].append(f"ğŸ’¾ Adequate memory ({memory_gb}GB) - full threading capacity available")
        
        return config
    
    def check_dependencies(self) -> Dict:
        """æ£€æŸ¥å…³é”®ä¾èµ–å’Œé…ç½®"""
        deps_info = {
            'ffmpeg_available': False,
            'ffmpeg_hardware_support': False,
            'whisper_model_available': False,
            'issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥FFmpeg
        try:
            result = subprocess.run(['ffmpeg', '-version'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                deps_info['ffmpeg_available'] = True
                
                # æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿæ”¯æŒ
                if 'videotoolbox' in result.stdout.lower():
                    deps_info['ffmpeg_hardware_support'] = True
                    deps_info['recommendations'].append("âœ… FFmpeg with VideoToolbox acceleration detected")
                else:
                    deps_info['issues'].append("âš ï¸ FFmpeg found but no hardware acceleration detected")
                    
        except Exception:
            deps_info['issues'].append("âŒ FFmpeg not found - video processing will fail")
        
        # æ£€æŸ¥Whisperæ¨¡å‹
        model_paths = [
            os.path.join(os.path.dirname(os.path.dirname(__file__)), "models", "ggml-distil-large-v3.5.bin"),
            os.path.expanduser("~/.whisper_models/ggml-distil-large-v3.5.bin")
        ]
        
        for model_path in model_paths:
            if os.path.exists(model_path):
                deps_info['whisper_model_available'] = True
                deps_info['recommendations'].append(f"âœ… Whisper model found at: {model_path}")
                break
        else:
            deps_info['issues'].append("âš ï¸ Whisper model not found - will download on first use")
        
        return deps_info
    
    def generate_optimization_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ä¼˜åŒ–æŠ¥å‘Š"""
        hw_accel = self.check_hardware_acceleration()
        thread_config = self.get_optimal_thread_config()
        deps_info = self.check_dependencies()
        
        report = []
        report.append("=" * 60)
        report.append("ğŸ”§ SYSTEM OPTIMIZATION REPORT")
        report.append("=" * 60)
        
        # ç³»ç»Ÿä¿¡æ¯
        report.append("\nğŸ“‹ SYSTEM INFORMATION:")
        report.append(f"  Platform: {self.system_info['platform']} ({self.system_info['architecture']})")
        report.append(f"  CPU: {self.system_info.get('cpu_model', 'Unknown')} ({self.system_info['cpu_count']} cores)")
        if 'memory_gb' in self.system_info:
            report.append(f"  Memory: {self.system_info['memory_gb']} GB")
        if self.system_info.get('is_apple_silicon'):
            report.append("  ğŸ Apple Silicon detected")
        
        # ç¡¬ä»¶åŠ é€Ÿ
        report.append("\nğŸš€ HARDWARE ACCELERATION:")
        if hw_accel['recommendations']:
            for rec in hw_accel['recommendations']:
                report.append(f"  {rec}")
        else:
            report.append("  âš ï¸ No hardware acceleration detected")
        
        # çº¿ç¨‹é…ç½®
        report.append("\nâš™ï¸ OPTIMAL THREAD CONFIGURATION:")
        report.append(f"  Whisper Processing: {thread_config['whisper_threads']} threads")
        report.append(f"  OpenAI Translation: {thread_config['translation_threads_openai']} threads")
        report.append(f"  Google Translation: {thread_config['translation_threads_google']} threads")
        report.append(f"  Main Thread Pool: {thread_config['main_thread_pool']} threads")
        
        if thread_config['reasoning']:
            report.append("\nğŸ’¡ OPTIMIZATION REASONING:")
            for reason in thread_config['reasoning']:
                report.append(f"  {reason}")
        
        # ä¾èµ–æ£€æŸ¥
        report.append("\nğŸ”— DEPENDENCIES STATUS:")
        if deps_info['recommendations']:
            for rec in deps_info['recommendations']:
                report.append(f"  {rec}")
        if deps_info['issues']:
            for issue in deps_info['issues']:
                report.append(f"  {issue}")
        
        # æ€§èƒ½å»ºè®®
        report.append("\nğŸ¯ PERFORMANCE RECOMMENDATIONS:")
        
        if self.system_info['cpu_count'] >= 8:
            report.append("  âœ… High-performance CPU detected - expect fast processing")
        elif self.system_info['cpu_count'] <= 4:
            report.append("  âš ï¸ Limited CPU cores - consider processing fewer files simultaneously")
        
        if self.system_info.get('memory_gb', 8) >= 16:
            report.append("  âœ… Adequate memory for concurrent processing")
        else:
            report.append("  âš ï¸ Limited memory - avoid processing too many large files at once")
        
        if hw_accel['metal_support']:
            report.append("  âœ… Apple Silicon GPU - optimal for AI workloads")
        elif not hw_accel['gpu_available']:
            report.append("  âš ï¸ No dedicated GPU - CPU-only processing expected")
        
        report.append("\n" + "=" * 60)
        
        return "\n".join(report)
    
    def get_optimized_config(self) -> Dict:
        """è·å–ä¼˜åŒ–åçš„é…ç½®å‚æ•°"""
        thread_config = self.get_optimal_thread_config()
        hw_accel = self.check_hardware_acceleration()
        
        return {
            'whisper_threads': thread_config['whisper_threads'],
            'openai_workers': thread_config['translation_threads_openai'],
            'google_workers': thread_config['translation_threads_google'],
            'main_pool_size': thread_config['main_thread_pool'],
            'use_hardware_accel': hw_accel['videotoolbox_support'],
            'system_info': self.system_info,
        }


def print_system_report():
    """ä¾¿æ·å‡½æ•°ï¼šæ‰“å°ç³»ç»Ÿä¼˜åŒ–æŠ¥å‘Š"""
    optimizer = SystemOptimizer()
    print(optimizer.generate_optimization_report())
    return optimizer.get_optimized_config()


if __name__ == "__main__":
    print_system_report()
